{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1. PyTorch Introduction.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75Ve9I3m0Rij"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = np.ones(6, dtype='int64')\n",
        "b = torch.from_numpy(a)\n",
        "print(f'a : {a}, a_dtype ={a.dtype} ')\n",
        "print(f'b : {b}, b_dtype ={b.dtype} ')"
      ],
      "metadata": {
        "id": "C6Zo9O_p00n2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e31be501-06b0-4cda-ea7b-6c20a310fccf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a : [1 1 1 1 1 1], a_dtype =int64 \n",
            "b : tensor([1, 1, 1, 1, 1, 1]), b_dtype =torch.int64 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor(3., requires_grad = True)\n",
        "y = torch.tensor(4., requires_grad=True)\n",
        "z = torch.tensor(5., requires_grad=True)\n",
        "k = x+y\n",
        "f = k * z\n",
        "\n",
        "f.backward()\n",
        "print(f'the grad of x w.r.t f is {x.grad}')\n",
        "print(f'the grad of y w.r.t f is {y.grad}')\n",
        "print(f'the grad of z w.r.t f is {z.grad}')\n",
        "print(f'the grad of k w.r.t f is {k.grad}')\n",
        "print(f'the grad of f w.r.t f is {f.grad}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TtXln3UcgGUq",
        "outputId": "0047e7d0-8704-48dc-8248-6565be5ede19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the grad of x w.r.t f is 5.0\n",
            "the grad of y w.r.t f is 5.0\n",
            "the grad of z w.r.t f is 7.0\n",
            "the grad of k w.r.t f is None\n",
            "the grad of f w.r.t f is None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/_tensor.py:1083: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at  aten/src/ATen/core/TensorBody.h:477.)\n",
            "  return self._grad\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## AUTOGRAD IN PYTORCH"
      ],
      "metadata": {
        "id": "1VJcEA41lA01"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor(1.0, requires_grad= True)\n",
        "y = torch.tensor(2.0)\n",
        "z = x * y\n",
        "\n",
        "for i, name in zip([x, y, z], 'xyz'):\n",
        "  print(f'{name}\\ndata: {i.data}\\nrequires_grad: {i.requires_grad}\\ngrad: {i.grad}\\ngrad_fn : {i.grad_fn}\\nis_leaf : {i.is_leaf}\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pW63a0F7npmq",
        "outputId": "f4b98c94-3a22-4873-ce18-8edb1644fef1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x\n",
            "data: 1.0\n",
            "requires_grad: True\n",
            "grad: None\n",
            "grad_fn : None\n",
            "is_leaf : True\n",
            "\n",
            "y\n",
            "data: 2.0\n",
            "requires_grad: False\n",
            "grad: None\n",
            "grad_fn : None\n",
            "is_leaf : True\n",
            "\n",
            "z\n",
            "data: 2.0\n",
            "requires_grad: True\n",
            "grad: None\n",
            "grad_fn : <MulBackward0 object at 0x7f71c4b7db90>\n",
            "is_leaf : False\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/_tensor.py:1083: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at  aten/src/ATen/core/TensorBody.h:477.)\n",
            "  return self._grad\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(5, requires_grad = True)\n",
        "y = x + 2\n",
        "z = x.mean()\n",
        "y.sum().backward()\n",
        "print(x.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dHEC6LgehxjJ",
        "outputId": "13daf67d-6f6b-485e-9936-e136c25cedd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 1., 1., 1., 1.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.arange(3., requires_grad=True)\n",
        "y= torch.dot(x,x)\n",
        "print(x)\n",
        "print(y)\n",
        "print(x.grad) # None since no operation has been rendered on the x\n",
        "y.backward()\n",
        "print(x.grad)\n",
        "2*x == x.grad\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1hz_IDcXlctt",
        "outputId": "97dc923a-ef03-46dd-b07a-b8bc270d9a64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0., 1., 2.], requires_grad=True)\n",
            "tensor(5., grad_fn=<DotBackward0>)\n",
            "None\n",
            "tensor([0., 2., 4.])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([True, True, True])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x.grad.zero_()\n",
        "y = x.sum()\n",
        "y.backward()\n",
        "print(x.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7TzDtXHVZn_T",
        "outputId": "12e2a2d0-8e7c-4ea7-a672-8176680213c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 1., 1.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.arange(4., requires_grad=True) #int grad error, Only tensors with floating point or complex dtype can require gradients\n",
        "y = x ** 2\n",
        "y.backward()\n",
        "print(x.grad) # Error shown why cos grad can be implicitly created for scalar outputs!!!!"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        },
        "id": "IcrFP-Iff6lD",
        "outputId": "dcdf59af-eed4-48f2-e2dd-bf0de41487af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-83e78c404cb4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#int grad error, Only tensors with floating point or complex dtype can require gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Error shown why !!!!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0mgrad_tensors_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tensor_or_tensors_to_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m     \u001b[0mgrad_tensors_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_grads_batched\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mretain_graph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"grad can be implicitly created only for scalar outputs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m                 \u001b[0mnew_grads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreserve_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.arange(4., requires_grad=True) #int grad error, Only tensors with floating point or complex dtype can require gradients\n",
        "y = x ** 2 # create a vector differentiation\n",
        "y.backward(torch.tensor([1., 1., 1., 1.]))\n",
        "x.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvMILp7WkuBA",
        "outputId": "2df35022-65a4-4a03-d0cd-5ecd05c845f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 2., 4., 6.])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## UNTRACK GRADIENT"
      ],
      "metadata": {
        "id": "D1Er-U4kxAHx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn((4,2), requires_grad = True)\n",
        "print(x)\n",
        "# x.requires_grad_(False)\n",
        "# y = x.detach()\n",
        "with torch.no_grad():\n",
        "  y = x+ 5\n",
        "  print(x)\n",
        "  print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQmLiykrxMJc",
        "outputId": "6eb575e7-ce97-4a1b-fb9c-f5de19b7fcc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.8625, -0.9797],\n",
            "        [ 0.7262,  0.3486],\n",
            "        [ 1.3243,  0.6804],\n",
            "        [ 0.4570,  0.0698]], requires_grad=True)\n",
            "tensor([[ 0.8625, -0.9797],\n",
            "        [ 0.7262,  0.3486],\n",
            "        [ 1.3243,  0.6804],\n",
            "        [ 0.4570,  0.0698]], requires_grad=True)\n",
            "tensor([[5.8625, 4.0203],\n",
            "        [5.7262, 5.3486],\n",
            "        [6.3243, 5.6804],\n",
            "        [5.4570, 5.0698]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradient Numpy"
      ],
      "metadata": {
        "id": "8Lc6FO92z6C0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array([1,2,3,4], dtype= np.float32)\n",
        "y = np.array([2,4,6,8], dtype = np.float32)\n",
        "\n",
        "w = 0.0\n",
        "\n",
        "# forward\n",
        "def forward(X):\n",
        "  return w * X\n",
        "\n",
        "def loss(y, y_pred):\n",
        "  return ((y_pred - y)**2).mean()\n",
        "\n",
        "def gradient(X, y, y_pred):\n",
        "  return np.dot(2*X, y_pred - y).mean()\n",
        "\n",
        "iters = 20\n",
        "lr = 0.01\n",
        "\n",
        "print(f'The prediction before training is {forward(5):.5f}')\n",
        "\n",
        "for epoch in range(iters):\n",
        "  # forward pass\n",
        "   y_pred = forward(X)\n",
        "\n",
        "   # loss\n",
        "   l = loss(y, y_pred)\n",
        "\n",
        "   # compute gradient\n",
        "   dw = gradient(X,y,y_pred)\n",
        "\n",
        "   w-= lr *  dw\n",
        "\n",
        "   if epoch % 2 == 0:\n",
        "     print(f\"epoch: {epoch+1}, weight: {w:.3f}, loss: {l:.8f}, gradient: {dw:.5f}\")\n",
        "\n",
        "print(f'The prediction after training is {forward(5):.5f}')"
      ],
      "metadata": {
        "id": "otlDm9cmxw2a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adca4654-7a63-43b6-f942-50e18daba00e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The prediction before training is 0.00000\n",
            "epoch: 1, weight: 1.200, loss: 30.00000000, gradient: -120.00000\n",
            "epoch: 3, weight: 1.872, loss: 0.76800019, gradient: -19.20000\n",
            "epoch: 5, weight: 1.980, loss: 0.01966083, gradient: -3.07200\n",
            "epoch: 7, weight: 1.997, loss: 0.00050331, gradient: -0.49152\n",
            "epoch: 9, weight: 1.999, loss: 0.00001288, gradient: -0.07864\n",
            "epoch: 11, weight: 2.000, loss: 0.00000033, gradient: -0.01258\n",
            "epoch: 13, weight: 2.000, loss: 0.00000001, gradient: -0.00202\n",
            "epoch: 15, weight: 2.000, loss: 0.00000000, gradient: -0.00032\n",
            "epoch: 17, weight: 2.000, loss: 0.00000000, gradient: -0.00005\n",
            "epoch: 19, weight: 2.000, loss: 0.00000000, gradient: -0.00001\n",
            "The prediction after training is 10.00000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GRADIENT WITH TORCH "
      ],
      "metadata": {
        "id": "BpHinxjS5GMa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.tensor([1,2,3,4], dtype= torch.float32)\n",
        "y = torch.tensor([2,4,6,8], dtype = torch.float32)\n",
        "\n",
        "w = torch.tensor(0.0, dtype=torch.float32, requires_grad= True)\n",
        "\n",
        "# forward\n",
        "def forward(X):\n",
        "  return w * X\n",
        "\n",
        "def loss(y, y_pred):\n",
        "  return ((y_pred - y)**2).mean()\n",
        "\n",
        "\n",
        "iters = 100\n",
        "lr = 0.01\n",
        "\n",
        "print(f'The prediction before training is {forward(5):.5f}')\n",
        "\n",
        "for epoch in range(iters):\n",
        "  y_pred = forward(X)\n",
        "  \n",
        "  l = loss(y, y_pred)\n",
        "  \n",
        "  \n",
        "  l.backward( )\n",
        "  \n",
        "  with torch.no_grad():\n",
        "     w -= lr * w.grad\n",
        "\n",
        "  w.grad.zero_()\n",
        "  \n",
        "  if epoch % 10 == 0:\n",
        "     print(f\"epoch: {epoch+1}, weight: {w:.3f}, loss: {l:.8f}, gradient: {w.grad:.5f}\")\n",
        "\n",
        "print(f'The prediction after training is {forward(5):.5f}')"
      ],
      "metadata": {
        "id": "CfmnDm-k3ECs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fa3fdf8-78ec-49f2-eea1-5b52bb0f536f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The prediction before training is 0.00000\n",
            "epoch: 1, weight: 0.300, loss: 30.00000000, gradient: 0.00000\n",
            "epoch: 11, weight: 1.665, loss: 1.16278565, gradient: 0.00000\n",
            "epoch: 21, weight: 1.934, loss: 0.04506890, gradient: 0.00000\n",
            "epoch: 31, weight: 1.987, loss: 0.00174685, gradient: 0.00000\n",
            "epoch: 41, weight: 1.997, loss: 0.00006770, gradient: 0.00000\n",
            "epoch: 51, weight: 1.999, loss: 0.00000262, gradient: 0.00000\n",
            "epoch: 61, weight: 2.000, loss: 0.00000010, gradient: 0.00000\n",
            "epoch: 71, weight: 2.000, loss: 0.00000000, gradient: 0.00000\n",
            "epoch: 81, weight: 2.000, loss: 0.00000000, gradient: 0.00000\n",
            "epoch: 91, weight: 2.000, loss: 0.00000000, gradient: 0.00000\n",
            "The prediction after training is 10.00000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PYTORCH DESIGN"
      ],
      "metadata": {
        "id": "CSFMWFAN9wyX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Design model (input, output, forward pass)\n",
        "# Construct loss and optimizer\n",
        "# Create a training loop\n",
        "#   - forward pass : compute prediction\n",
        "#   - backward pass: compute gradient\n",
        "#   - optimizer: update weights\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "X = torch.tensor([[1],[2],[3],[4]], dtype= torch.float32)\n",
        "y = torch.tensor([[2],[4],[6],[8]], dtype = torch.float32)\n",
        "\n",
        "X_test = torch.tensor([5], dtype = torch.float32)\n",
        "\n",
        "n_samples, n_features = X.shape\n",
        "\n",
        "input_size = n_features\n",
        "output_size = n_features\n",
        "\n",
        "\n",
        "model = nn.Linear(input_size, output_size)\n",
        "\n",
        "iters = 1000\n",
        "lr = 0.01\n",
        "\n",
        "w = torch.tensor(0.0, dtype=torch.float32, requires_grad= True)\n",
        "\n",
        "loss = nn.MSELoss()\n",
        "\n",
        "optimizer= torch.optim.SGD(model.parameters(), lr = lr)\n",
        "\n",
        "print(f'The prediction before training is {model(X_test).item():.5f}')\n",
        "\n",
        "for epoch in range(iters):\n",
        "  y_pred = model(X)\n",
        "  \n",
        "  l = loss(y, y_pred)\n",
        "  \n",
        "  #gradient backward pass\n",
        "  l.backward( )\n",
        "\n",
        "  #update weights\n",
        "  optimizer.step()\n",
        "\n",
        "  #  zero the gradients\n",
        "  optimizer.zero_grad()\n",
        "  \n",
        "  if epoch % 100 == 0:\n",
        "    [w, b] = model.parameters()\n",
        "    print(f\"epoch: {epoch+1}, weight: {w[0][0].item():.3f}, loss: {l:.8f}\")\n",
        "\n",
        "print(f'The prediction after training for Model(5):- {model(X_test).item():.5f}')\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7afct89a7LRq",
        "outputId": "124bc640-db51-46ed-bcb5-510d6cd36d00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The prediction before training is 1.98644\n",
            "epoch: 1, weight: 0.424, loss: 16.44056892\n",
            "epoch: 101, weight: 1.644, loss: 0.18391101\n",
            "epoch: 201, weight: 1.736, loss: 0.10096490\n",
            "epoch: 301, weight: 1.805, loss: 0.05542856\n",
            "epoch: 401, weight: 1.855, loss: 0.03042961\n",
            "epoch: 501, weight: 1.893, loss: 0.01670550\n",
            "epoch: 601, weight: 1.921, loss: 0.00917110\n",
            "epoch: 701, weight: 1.941, loss: 0.00503483\n",
            "epoch: 801, weight: 1.956, loss: 0.00276407\n",
            "epoch: 901, weight: 1.968, loss: 0.00151743\n",
            "The prediction after training for Model(5):- 9.95052\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# custom Linear model class\n",
        "\n",
        "class LinearModel(nn.Module):\n",
        "  \n",
        "  def __init__(self, input_dim, output_dim):\n",
        "\n",
        "    super(LinearModel, self).__init__()\n",
        "\n",
        "    self.lin = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    return self.lin(x)\n",
        "\n",
        "X = torch.tensor([[1],[2],[3],[4]], dtype= torch.float32)\n",
        "y = torch.tensor([[2],[4],[6],[8]], dtype = torch.float32)\n",
        "\n",
        "X_test = torch.tensor([5], dtype = torch.float32)\n",
        "n_samples, n_features = X.shape\n",
        "\n",
        "input_size = n_features\n",
        "output_size = n_features\n",
        "\n",
        "\n",
        "model = LinearModel(input_size, output_size)\n",
        "\n",
        "loss = nn.MSELoss()\n",
        "\n",
        "optimizer= torch.optim.SGD(model.parameters(), lr = lr)\n",
        "\n",
        "print(f'The prediction before training for  Model(5) :- {model(X_test).item():.5f}')\n",
        "\n",
        "iters = 1000\n",
        "lr = 0.01\n",
        "\n",
        "for epoch in range(iters):\n",
        "  y_pred = model(X)\n",
        "  \n",
        "  l = loss(y, y_pred)\n",
        "  \n",
        "  #gradient backward pass\n",
        "  l.backward( )\n",
        "\n",
        "  #update weights\n",
        "  optimizer.step()\n",
        "\n",
        "  #  zero the gradients\n",
        "  optimizer.zero_grad()\n",
        "  \n",
        "  if epoch % 100 == 0:\n",
        "    [w, b] = model.parameters()\n",
        "\n",
        "    print(f\"epoch: {epoch+1}, weight: {w[0][0].item():.3f}, bias: {b[0]:.3f}, loss: {l:.8f}, \")\n",
        "\n",
        "print(f'The prediction after training for Model(5):- {model(X_test).item():.5f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vLaVEcjhD39w",
        "outputId": "c6ec5b79-32eb-42b4-f07a-7a5baa42bfe5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The prediction before training for  Model(5) :- 3.49857\n",
            "epoch: 1, weight: 0.696, bias: 0.957, loss: 10.58054352, \n",
            "epoch: 101, weight: 1.684, bias: 0.930, loss: 0.14536212, \n",
            "epoch: 201, weight: 1.766, bias: 0.689, loss: 0.07980205, \n",
            "epoch: 301, weight: 1.826, bias: 0.511, loss: 0.04381045, \n",
            "epoch: 401, weight: 1.871, bias: 0.378, loss: 0.02405139, \n",
            "epoch: 501, weight: 1.905, bias: 0.280, loss: 0.01320392, \n",
            "epoch: 601, weight: 1.929, bias: 0.208, loss: 0.00724879, \n",
            "epoch: 701, weight: 1.948, bias: 0.154, loss: 0.00397950, \n",
            "epoch: 801, weight: 1.961, bias: 0.114, loss: 0.00218470, \n",
            "epoch: 901, weight: 1.971, bias: 0.084, loss: 0.00119937, \n",
            "The prediction after training for Model(5):- 9.95601\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "\n",
        "X_numpy, y_numpy = datasets.make_regression(n_samples = 100, n_features=1, noise=20, random_state=4)\n",
        "X = torch.from_numpy(X_numpy.astype(np.float32))\n",
        "y_t = torch.from_numpy(y_numpy.astype(np.float32))\n",
        "y = y_t.view(y_t.shape[0], 1)\n",
        "\n",
        "n_sample, n_features = X.shape\n",
        "input_size = n_features\n",
        "output_size = 1\n",
        "model = nn.Linear(input_size, output_size)\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "opt = torch.optim.SGD(model.parameters(), lr = lr)\n",
        "\n",
        "lr = 0.01\n",
        "num_epochs = 100\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  prediction = model(X)\n",
        "\n",
        "  loss = criterion(prediction, y)\n",
        "\n",
        "  loss.backward()\n",
        "\n",
        "  opt.step()\n",
        "\n",
        "  opt.zero_grad()\n",
        "\n",
        "  #if epoch % 1 == 0:\n",
        "    #print(f'epoch {epoch+1}- loss: {loss.item():.4f}')\n",
        "\n",
        "predictions = model(X).detach().numpy()\n",
        "\n",
        "plt.plot(X_numpy, y_numpy, 'ro')\n",
        "plt.plot(X_numpy, predictions, 'g')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wE4kieAzFJnQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "e5c32a55-057f-49c1-ce13-63b99c2c0934"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "angDLJWiLDdg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}